ğŸ§  RÃ©sumÃ© du projet : Reinforcement Learning from Human Feedback (RLHF) in Notebooks

Ce projet propose une implÃ©mentation pÃ©dagogique et pas Ã  pas du framework RLHF (Reinforcement Learning from Human Feedback) Ã  lâ€™aide de Jupyter Notebooks. Lâ€™objectif est de montrer concrÃ¨tement comment aligner un modÃ¨le de langage prÃ©-entraÃ®nÃ© (GPT-2) avec des prÃ©fÃ©rences humaines en combinant apprentissage supervisÃ©, apprentissage par renforcement et modÃ¨les de rÃ©compense.

ğŸ¯ Objectif principal

Adapter un modÃ¨le GPT-2 prÃ©-entraÃ®nÃ© afin quâ€™il gÃ©nÃ¨re uniquement des phrases Ã  sentiment positif, en sâ€™inspirant du principe du RLHF utilisÃ© pour aligner des grands modÃ¨les de langage comme ChatGPT.

ğŸ”„ Principe du RLHF

Le RLHF est une mÃ©thode dâ€™alignement des modÃ¨les de langage oÃ¹ le feedback humain nâ€™est pas utilisÃ© directement comme rÃ©compense, mais sert Ã  entraÃ®ner un modÃ¨le de rÃ©compense. Ce modÃ¨le est ensuite utilisÃ© dans une boucle de reinforcement learning.

Le processus se dÃ©roule en trois Ã©tapes clÃ©s :

Supervised Fine-Tuning (SFT)

Reward Model Training (RM)

Reinforcement Learning avec PPO

ğŸ“Š Cas dâ€™Ã©tude du projet

Au lieu de crÃ©er un chatbot avec des rÃ©ponses classÃ©es par des humains, le projet simplifie le cadre RLHF en utilisant le dataset stanfordnlp/sst2, composÃ© de phrases de critiques de films annotÃ©es comme positives ou nÃ©gatives.

ğŸ‘‰ Le rÃ´le du feedback humain est ici simulÃ© par les labels de sentiment.

ğŸ“’ Contenu des notebooks
1ï¸âƒ£ 1-SFT.ipynb â€“ Supervised Fine-Tuning

GPT-2 est fine-tunÃ© de maniÃ¨re supervisÃ©e sur le dataset SST-2.

Le modÃ¨le apprend Ã  gÃ©nÃ©rer des phrases similaires aux critiques de films.

Le modÃ¨le obtenu est appelÃ© SFT model.

2ï¸âƒ£ 2-RM Training.ipynb â€“ Reward Model

Un modÃ¨le de rÃ©compense est construit en ajoutant une reward head Ã  GPT-2.

Ce modÃ¨le est entraÃ®nÃ© pour prÃ©dire le sentiment (positif ou nÃ©gatif) dâ€™une phrase.

Il simule lâ€™Ã©valuation humaine de la qualitÃ© des gÃ©nÃ©rations.

3ï¸âƒ£ 3-RLHF.ipynb â€“ Reinforcement Learning avec PPO

Le modÃ¨le SFT est utilisÃ© comme policy initiale.

Il gÃ©nÃ¨re des phrases qui sont Ã©valuÃ©es par le reward model.

Lâ€™algorithme PPO (Proximal Policy Optimization) ajuste les paramÃ¨tres du modÃ¨le pour maximiser la rÃ©compense (sentiment positif).

âœ… RÃ©sultat final

AprÃ¨s lâ€™entraÃ®nement RLHF :

GPT-2 est alignÃ© pour produire majoritairement des phrases Ã  sentiment positif.

Le projet dÃ©montre de faÃ§on concrÃ¨te comment RLHF peut guider un LLM vers un comportement souhaitÃ©.


comprendre le fonctionnement interne de ChatGPT-like models

Excellent pont entre NLP, Deep Learning et Reinforcement Learning
